# Summary of attention_is_all_you_need

## Title
Attention Is All You Need

## Authors
- **

## Abstract
The Transformer is a model architecture that relies entirely on an attention mechanism to draw global dependencies between input and output. This allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs.

## Sections
- Attention Is All You Need (Level 1)
- Title: Attention is All You Need (Level 1)
- Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Stephan Gouws, Yoshua Bengio, and Ian J. Goodfellow (Level 2)
- Abstract (Level 2)
- 1 Introduction (Level 2)
- 2 Background (Level 2)
- 3 Model Architecture (Level 2)
- The Transformer: Model Architecture (Level 1)
- 3.1 Encoder and Decoder Stacks (Level 2)
- Encoder: (Level 3)
- Decoder: (Level 3)
- 3.2 Attention (Level 2)
- Scaled Dot-Product Attention and Multi-Head Attention (Level 1)
- Figure 2 (Level 2)
- 3.2.1 Scaled Dot-Product Attention (Level 2)
- 3.2.2 Multi-Head Attention (Level 2)
- Title: Attention is All You Need (Level 1)
- Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Stephan Gouws, Yoshua Bengio, and Ian J. Goodfellow (Level 2)
- Abstract (Level 2)
- 3.2.2 Multi-Head Attention (Level 2)
- 3.2.3 Applications of Attention in our Model (Level 2)
- 3.3 Position-wise Feed-Forward Networks (Level 2)
- 3.4 Embeddings and Softmax (Level 2)
- Title: [Title of the Paper] (Level 1)
- Authors: [Author Names] (Level 2)
- Abstract (Level 2)
- 3.5 Positional Encoding (Level 2)
- 4 Why Self-Attention (Level 2)
- Table 1: Maximum path lengths, per-layer complexity and minimum number of sequential operations for different layer types. (Level 2)
- Title: [Title of the Paper] (Level 1)
- Authors: [Author Names] (Level 2)
- Abstract (Level 2)
- 1. Introduction (Level 2)
- 2. Background (Level 2)
- 3. Model Architecture (Level 2)
- 3.1 Self-Attention (Level 3)
- 5. Training (Level 2)
- 5.1 Training Data and Batching (Level 3)
- 5.2 Hardware and Schedule (Level 3)
- 5.3 Optimizer (Level 3)
- 5.4 Regularization (Level 3)
- References (Level 2)
- Appendix (Level 2)
- Title: Attention is All You Need (Level 1)
- Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Stephan Gouws, Yoshua Bengio, and Ian J. Goodfellow (Level 2)
- Abstract (Level 2)
- 1 Introduction (Level 2)
- 2 Background (Level 2)
- 3 The Transformer (Level 2)
- 3.1 Architecture (Level 3)
- 3.2 Self-Attention (Level 3)
- 3.3 Positional Encoding (Level 3)
- 4 Training (Level 2)
- 4.1 Training Objective (Level 3)
- 4.2 Optimization (Level 3)
- 5 Experiments (Level 2)
- 5.1 Datasets (Level 3)
- 5.2 Evaluation Metrics (Level 3)
- 6 Results (Level 2)
- 6.1 Machine Translation (Level 3)
- 6.2 Model Variations (Level 3)
- Table 2: The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. (Level 2)
- Footnotes (Level 2)
- Title: Variations on the Transformer Architecture (Level 1)
- Authors: [Authors' names would be here] (Level 2)
- Abstract (Level 2)
- 6.3 English Constituency Parsing (Level 2)
- Table 3: Variations on the Transformer architecture (Level 2)
- Title: The Transformer: Attention is All You Need (Level 1)
- Authors: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Stephan Gouws, Yoshua Bengio, and Ian J. Goodfellow (Level 2)
- Abstract (Level 2)
- 7 Conclusion (Level 2)
- Acknowledgements (Level 2)
- References (Level 2)
- Table 4: The Transformer generalizes well to English constituency parsing (Results are on Section 23 of WSJ) (Level 2)
- Title: Learning Phrase Representations Using RNN Encoder-Decoder for Statistical Machine Translation (Level 1)
- Authors (Level 2)
- Abstract (Level 2)
- References (Level 2)
- Footnotes (Level 2)
- References (Level 2)
- Attention Visualizations (Level 1)

## References
0 references found

